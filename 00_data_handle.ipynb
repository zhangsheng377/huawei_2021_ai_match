{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36bf560-6349-4b9e-9390-6e6783124985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from textrank4zh import TextRank4Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063f19a4-bb30-4976-a1af-4d6617ef48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_test_path = \"2021_2_data/doc_quality_data_test.json\"\n",
    "origin_train_path = \"2021_2_data/doc_quality_data_train.json\"\n",
    "\n",
    "processed_test_path = \"2021_2_data/processed_test.json\"\n",
    "processed_train_path = \"2021_2_data/processed_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ade4a2-2ca0-463e-832b-c9cfcc514993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_upprintable_chars(s):\n",
    "    \"\"\"移除所有不可见字符\"\"\"\n",
    "    return ''.join(x for x in s if x.isprintable() or x=='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3db5ab4-78f2-456a-8955-f57017b96da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_list(body):\n",
    "    parser_body_list_ = [remove_upprintable_chars(s) for s in BeautifulSoup(body, 'html.parser')._all_strings()]\n",
    "    parser_body_str = \"\".join(parser_body_list_)\n",
    "    parser_body_list = []\n",
    "    for ss in parser_body_str.split('\\n'):\n",
    "        ss = ss.strip()\n",
    "        if len(ss) > 0 and \\\n",
    "        not (len(ss)<30 and ss[0] in ['（', '('] and ss[-1] in ['）', ')']) and \\\n",
    "        not (len(ss)<50 and (ss.startswith(\"资料图\") or ss.startswith(\"图\"))):\n",
    "            parser_body_list.append(ss)\n",
    "    return parser_body_list, len(parser_body_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa882fbf-649f-4f30-bed6-35d1c9874caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remove_title_paragraphs_list(body, title):\n",
    "    parser_body_list, body_len = get_paragraphs_list(body)\n",
    "    end_index = 1\n",
    "    # 大于1说明有html标签\n",
    "    if body_len > 1:\n",
    "        end_index = 2\n",
    "    paragraphs_list = []\n",
    "    # 检查 标题是否出现在前两个元素中 (有可能存在标签<p class=\\\"ori_titlesource\\\">,会有\"原标题: title\"的情况出现)\n",
    "    for sentence in parser_body_list[:end_index]:\n",
    "        for _ in range(2):\n",
    "            title_index = sentence.find(title)\n",
    "            if title_index > -1 and title_index < 2 * len(title):\n",
    "                sentence = sentence[title_index + len(title) :]\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) > 0:\n",
    "            paragraphs_list.append(sentence)\n",
    "    for sentence in parser_body_list[end_index:]:\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) > 0:\n",
    "            paragraphs_list.append(sentence)\n",
    "    return paragraphs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f503efe1-cca3-42ec-bc4f-4d9ca1288039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(paragraphs_list):\n",
    "    tmp = paragraphs_list[:2]\n",
    "    if len(tmp) > 2:\n",
    "        tmp.append(paragraphs_list[-1])\n",
    "    char_num = 6\n",
    "    source = \"\"\n",
    "    for sentence in tmp:\n",
    "        # print(f\"get_source:{sentence}\")\n",
    "        source_index = sentence[:char_num].find(\"社\")\n",
    "        if source_index > -1:\n",
    "            source = sentence[:source_index+1]\n",
    "            break\n",
    "        source_index = sentence[:char_num].find(\"网\")\n",
    "        if source_index > -1:\n",
    "            source = sentence[:source_index+1]\n",
    "            break\n",
    "        source_index = sentence[:char_num].find(\"报\")\n",
    "        if source_index > -1:\n",
    "            source = sentence[:source_index+1]\n",
    "            break\n",
    "        source_index = sentence[:char_num].find(\"刊\")\n",
    "        if source_index > -1:\n",
    "            source = sentence[:source_index+1]\n",
    "            break\n",
    "        source_index = sentence[:char_num].find(\"讯\")\n",
    "        if source_index > -1:\n",
    "            source = sentence[:source_index]\n",
    "            break\n",
    "            \n",
    "    if source.startswith('文/'):\n",
    "        source = source[2:]\n",
    "    if source.startswith('来源：'):\n",
    "        source = source[3:]\n",
    "    if source.startswith('图源：'):\n",
    "        source = source[3:]\n",
    "        \n",
    "    index = source.find(\"，\")\n",
    "    if index > -1:\n",
    "        source = source[index+1:]\n",
    "    index = source.find(\"。\")\n",
    "    if index > -1:\n",
    "        source = source[index+1:]\n",
    "    index = source.find(\"：\")\n",
    "    if index > -1:\n",
    "        source = source[index+1:]\n",
    "    index = source.find(\"据\")\n",
    "    if index > -1:\n",
    "        source = source[index+1:]\n",
    "    index = source.find(\"从\")\n",
    "    if index > -1:\n",
    "        source = source[index+1:]\n",
    "        \n",
    "    source = remove_upprintable_chars(source)\n",
    "    \n",
    "    source = source.replace(\"】\", \"\")\n",
    "    source = source.replace(\"【\", \"\")\n",
    "    source = source.replace(\"]\", \"\")\n",
    "    source = source.replace(\"[\", \"\")\n",
    "    source = source.replace(\"（\", \"\")\n",
    "    source = source.replace(\"）\", \"\")\n",
    "    source = source.replace(\"(\", \"\")\n",
    "    source = source.replace(\")\", \"\")\n",
    "    source = source.replace(\"◎\", \"\")\n",
    "    source = source.replace(\"■\", \"\")\n",
    "    source = source.replace(\"□\", \"\")\n",
    "    source = source.replace(\"。\", \"\")\n",
    "    source = source.replace(\"*\", \"\")\n",
    "    source = source.replace(\"◆\", \"\")\n",
    "    source = source.replace(\"@\", \"\")\n",
    "    source = source.replace(\"《\", \"\")\n",
    "    source = source.replace(\"》\", \"\")\n",
    "    source = source.replace(\"<\", \"\")\n",
    "    source = source.replace(\">\", \"\")\n",
    "    source = source.replace(\"“\", \"\")\n",
    "    source = source.replace(\"”\", \"\")\n",
    "    source = source.replace(\"\\\"\", \"\")\n",
    "    source = source.replace(\"#\", \"\")\n",
    "    source = source.replace(\"△\", \"\")\n",
    "    source = source.replace(\"近日\", \"\")\n",
    "    \n",
    "    source = source.strip()\n",
    "    \n",
    "    if len(source) <=1:\n",
    "        source = \"\"\n",
    "        \n",
    "    if source in ['对于篮网', '有网', '本社', '本网', '本报', '本刊', '当今社', '不少网', '该报', '人社', '很多网', '在互联网', '北极星售电网', '美国媒体报', '此前报', '海报', '台湾媒体报',\n",
    "                  '最近有网', '图片来自网', '现在网', '据港媒报', '天猫目前报', '从网', '国外媒体报', '随着现在社', '具体预报', '在现代社', '中国政府网', '通报', '针对网', '现代社', '核心提示：报',\n",
    "                  '据报']:\n",
    "        source = \"\"\n",
    "        \n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52dfdd1e-63be-4ed3-a721-58861aefca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_sentences(text, max_len=500):\n",
    "    tr4s = TextRank4Sentence()\n",
    "    tr4s.analyze(text=text, lower=True, source = 'all_filters')\n",
    "\n",
    "    key_sentences = \"\"\n",
    "    for item in tr4s.get_key_sentences(num=9999):\n",
    "        # print(item.index, item.weight, item.sentence)  # index是语句在文本中位置，weight是权重\n",
    "        key_sentences += item.sentence + \"。\"\n",
    "        if len(key_sentences) > 500:\n",
    "            break\n",
    "    return key_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a956c82f-7b93-43bb-b739-62cce81005a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_count(input_file_path):\n",
    "    count = 0\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c30779-b8d7-44b5-b74b-eb95f8afe8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_origin_file(origin_path, processed_path):\n",
    "    origin_path_line_count = get_line_count(origin_path)\n",
    "    with open(origin_path, 'r', encoding='utf-8') as origin_file, open(processed_path, 'w', encoding='utf-8') as processed_file:\n",
    "        for line in tqdm(origin_file, total=origin_path_line_count):\n",
    "            json_data = json.loads(line)\n",
    "\n",
    "            title = remove_upprintable_chars(json_data['title'].strip())\n",
    "            paragraphs_list = get_remove_title_paragraphs_list(json_data['body'], title)\n",
    "            paragraphs_str = \"\\n\".join(paragraphs_list)\n",
    "            paragraphs_num = len(paragraphs_list)\n",
    "            pic_num = json_data['body'].count('<img ')\n",
    "            source = get_source(paragraphs_list)\n",
    "            key_sentences = get_key_sentences(paragraphs_str)\n",
    "\n",
    "            json_data['process_title'] = title\n",
    "            json_data['process_body'] = paragraphs_str\n",
    "            json_data['paragraphs_num'] = paragraphs_num\n",
    "            json_data['pic_num'] = pic_num\n",
    "            json_data['source'] = source\n",
    "            json_data['key_sentences'] = key_sentences\n",
    "\n",
    "            processed_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")\n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2888054f-b0de-4885-828d-e7629aa711ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45285 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.191 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "  0%|          | 0/45285 [00:01<?, ?it/s]\n",
      "  0%|          | 0/576454 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "process_origin_file(origin_test_path, processed_test_path)\n",
    "process_origin_file(origin_train_path, processed_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71474e0-c827-40a5-a7f5-fd224073eb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
