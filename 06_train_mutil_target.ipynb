{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5g1l-MySv13",
    "outputId": "717a5728-a74a-44f1-d47d-c0510945e390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 28 22:36:26 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 457.49       Driver Version: 457.49       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P8     5W /  N/A |    777MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1684    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      3148    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      7584    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     12916    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     15948    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     21860    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     26888    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     56104    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     90856    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     91160    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A    103372    C+G   ...root\\Office16\\WINWORD.EXE    N/A      |\n",
      "|    0   N/A  N/A    114748    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A    114776    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UetA7O1OU3ju"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nlKDWXwdVIIM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7sb1mTUJVLYz"
   },
   "outputs": [],
   "source": [
    "processed_expose_test_path = \"data/processed_test_expose.json\"\n",
    "processed_expose_train_labeled_path = \"data/processed_train_expose_labeled.json\"\n",
    "processed_expose_train_unlabel_predict_other_path = \"data/processed_train_expose_unlabel_predict_other_0.5.json\"\n",
    "processed_expose_train_valid_path = \"data/processed_train_valid.json\"\n",
    "processed_expose_train_train_path = \"data/processed_train_train.json\"\n",
    "\n",
    "pretrained_bert_path = \"bert-base-chinese/\"\n",
    "\n",
    "category_encoder_path = 'model/category_encoder_second.pickle'\n",
    "paragraphs_num_encoder_path = 'model/paragraphs_num_encoder_second.pickle'\n",
    "source_encoder_path = 'model/source_encoder_second.pickle'\n",
    "doctype_encoder_path = 'model/doctype_encoder_second.pickle'\n",
    "words_len_encoder_path = 'model/words_len_encoder_second.pickle'\n",
    "model_train_mutil_model_path = 'model/model_train_mutil_target.pt'\n",
    "\n",
    "submission_path = \"submission_train_mutil_target_predict.csv\"\n",
    "\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xvyXmwjiVwld"
   },
   "outputs": [],
   "source": [
    "def get_feature_list(input_path, feature_name):\n",
    "    feature_list = []\n",
    "    with open(input_path, 'r', encoding=\"utf-8\") as input_file:\n",
    "        for line in tqdm(input_file):\n",
    "            json_data = json.loads(line)\n",
    "            feature_list.append(json_data[feature_name])\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0jZkH8nNVzKW"
   },
   "outputs": [],
   "source": [
    "def get_category_encoder():\n",
    "    if os.path.exists(category_encoder_path):\n",
    "        with open (category_encoder_path, 'rb') as category_encoder_file: \n",
    "            return pickle.load(category_encoder_file)\n",
    "    \n",
    "    category_list = get_feature_list(processed_expose_train_labeled_path, 'category') + \\\n",
    "                    get_feature_list(processed_expose_test_path, 'category') + \\\n",
    "                    get_feature_list(processed_expose_train_unlabel_predict_other_path, 'category')\n",
    "    category_list = np.array(category_list).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore').fit(category_list)\n",
    "    \n",
    "    with open (category_encoder_path, 'wb') as category_encoder_file:\n",
    "        pickle.dump(encoder, category_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xgSN3_bV2Oo",
    "outputId": "6669d666-e358-4831-987e-fcd26dabe8ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_encoder = get_category_encoder()\n",
    "category_encoder.transform(np.array([1]).reshape(-1, 1)).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDMalpnWV54s",
    "outputId": "0346b08b-9962-4b7e-b5cc-f2e8e8006389"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "obSOBSJgV9HM"
   },
   "outputs": [],
   "source": [
    "def get_paragraphs_num_encoder():\n",
    "    if os.path.exists(paragraphs_num_encoder_path):\n",
    "        with open (paragraphs_num_encoder_path, 'rb') as paragraphs_num_encoder_file: \n",
    "            return pickle.load(paragraphs_num_encoder_file)\n",
    "    \n",
    "    paragraphs_num_list = get_feature_list(processed_expose_train_labeled_path, 'paragraphs_num') + \\\n",
    "                          get_feature_list(processed_expose_test_path, 'paragraphs_num') + \\\n",
    "                          get_feature_list(processed_expose_train_unlabel_predict_other_path, 'paragraphs_num')\n",
    "    paragraphs_num_list = np.array(paragraphs_num_list).reshape(-1, 1)\n",
    "    encoder = StandardScaler().fit(paragraphs_num_list)\n",
    "    \n",
    "    with open (paragraphs_num_encoder_path, 'wb') as paragraphs_num_encoder_file:\n",
    "        pickle.dump(encoder, paragraphs_num_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J0gKWqDV_Hy",
    "outputId": "38441f14-d57d-4e9c-b637-d7adf35c663e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.698883781221176"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_num_encoder = get_paragraphs_num_encoder()\n",
    "paragraphs_num_encoder.transform(np.array([100]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KAcTDhsWB_p",
    "outputId": "93b675f1-a6d7-4de4-e49e-1e590394b713"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6058788676.516701"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_num_encoder.transform(np.array([99999999999]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3wN3dvYs_cj3"
   },
   "outputs": [],
   "source": [
    "def get_words_len_encoder():\n",
    "    if os.path.exists(words_len_encoder_path):\n",
    "        with open (words_len_encoder_path, 'rb') as words_len_encoder_file: \n",
    "            return pickle.load(words_len_encoder_file)\n",
    "    \n",
    "    words_len_list = get_feature_list(processed_expose_train_labeled_path, 'words_len') + \\\n",
    "                     get_feature_list(processed_expose_test_path, 'words_len') + \\\n",
    "                     get_feature_list(processed_expose_train_unlabel_predict_other_path, 'words_len')\n",
    "    words_len_list = np.array(words_len_list).reshape(-1, 1)\n",
    "    encoder = StandardScaler().fit(words_len_list)\n",
    "    \n",
    "    with open (words_len_encoder_path, 'wb') as words_len_encoder_file:\n",
    "        pickle.dump(encoder, words_len_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5s7Mmgd_gVA",
    "outputId": "9d59c3cd-19fe-4f54-abb3-025f4b8517e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.39012417029706364"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_len_encoder = get_words_len_encoder()\n",
    "words_len_encoder.transform(np.array([1000]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RO6ZkBX_kLH",
    "outputId": "de4535dc-dd1a-47c5-9f81-e52cb0bc4907"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4138137636721993"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_len_encoder.transform(np.array([2000]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uW0geYy1WEmb"
   },
   "outputs": [],
   "source": [
    "def get_source_encoder():\n",
    "    if os.path.exists(source_encoder_path):\n",
    "        with open (source_encoder_path, 'rb') as source_encoder_file: \n",
    "            return pickle.load(source_encoder_file)\n",
    "    \n",
    "    source_list = get_feature_list(processed_expose_train_labeled_path, 'source') + \\\n",
    "                  get_feature_list(processed_expose_test_path, 'source') + \\\n",
    "                  get_feature_list(processed_expose_train_unlabel_predict_other_path, 'source')\n",
    "    source_list = np.array(source_list).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore').fit(source_list)\n",
    "    \n",
    "    with open (source_encoder_path, 'wb') as source_encoder_file:\n",
    "        pickle.dump(encoder, source_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fD1CoDpiWHDf",
    "outputId": "6e4eddef-9bcb-4d70-bf6a-6c7d5ece2803"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_encoder = get_source_encoder()\n",
    "source_encoder.transform(np.array(['中国经济周刊']).reshape(-1, 1)).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rSifyLaWKd5",
    "outputId": "3c088af8-e2a8-46bc-c32d-472e5c34474a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_encoder.transform(np.array(['hg']).reshape(-1, 1)).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpW8Ia_SWNhL",
    "outputId": "b3cbd0bd-fefc-478e-b102-d0113c0c2544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2b99VHK1WP-T"
   },
   "outputs": [],
   "source": [
    "def get_doctype_encoder():\n",
    "    if os.path.exists(doctype_encoder_path):\n",
    "        with open (doctype_encoder_path, 'rb') as doctype_encoder_file: \n",
    "            return pickle.load(doctype_encoder_file)\n",
    "    \n",
    "    doctype_list = get_feature_list(processed_expose_train_labeled_path, 'doctype') + \\\n",
    "                  get_feature_list(processed_expose_train_unlabel_predict_other_path, 'doctype')\n",
    "    doctype_set = set(doctype_list)\n",
    "    doctype_list = list(doctype_set)\n",
    "    \n",
    "    with open (doctype_encoder_path, 'wb') as doctype_encoder_file:\n",
    "        pickle.dump(doctype_list, doctype_encoder_file)\n",
    "    return doctype_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKsMBeGYWSXT",
    "outputId": "14f57c56-76ec-43ac-ae66-6419e3917149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['情感解读',\n",
       "  '深度事件',\n",
       "  '其他',\n",
       "  '人物专栏',\n",
       "  '攻略文',\n",
       "  '推荐文',\n",
       "  '治愈系文章',\n",
       "  '物品评测',\n",
       "  '行业解读',\n",
       "  '科普知识文',\n",
       "  '作品分析'],\n",
       " 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctype_list = get_doctype_encoder()\n",
    "doctype_list, len(doctype_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxxfHEhE_vvN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aqRPWxxo_vzh"
   },
   "outputs": [],
   "source": [
    "# with open(processed_expose_train_labeled_path, 'r', encoding=\"utf-8\") as input_file, \\\n",
    "#      open(processed_expose_train_unlabel_predict_other_path, 'r', encoding=\"utf-8\") as input1_file, \\\n",
    "#      open(processed_expose_train_train_path, 'w', encoding=\"utf-8\") as train_file, \\\n",
    "#      open(processed_expose_train_valid_path, 'w', encoding=\"utf-8\") as valid_file:\n",
    "#     for line in tqdm(input_file):\n",
    "#         json_data = json.loads(line)\n",
    "#         if random.random() < 0.1:\n",
    "#             valid_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")\n",
    "#         else:\n",
    "#             train_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")\n",
    "#     for line in tqdm(input1_file):\n",
    "#         json_data = json.loads(line)\n",
    "#         if random.random() < 0.1:\n",
    "#             valid_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")\n",
    "#         else:\n",
    "#             train_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdkeBfBeWYLR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7FIdsWBCWYPi"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, input_paths, dataset_type):\n",
    "        self.dataset_type = dataset_type\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_path)\n",
    "        self.data_list = self.load_data(input_paths)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_data_label(cls, tokenizer, item, dataset_type):\n",
    "        token = tokenizer(item['text'], add_special_tokens=True,\n",
    "                                              max_length=512,\n",
    "                                              truncation=True,\n",
    "                                              padding='max_length',\n",
    "                                              return_tensors=\"pt\")\n",
    "        del item['text']\n",
    "#         item = dict(item, **token)\n",
    "        item['input_ids'] = token['input_ids'][0]\n",
    "        item['token_type_ids'] = token['token_type_ids'][0]\n",
    "        item['attention_mask'] = token['attention_mask'][0]\n",
    "        \n",
    "        item['category'] = category_encoder.transform(np.array([item['category']]).reshape(-1, 1)).toarray()[0]\n",
    "        item['paragraphs_num'] = paragraphs_num_encoder.transform(np.array([item['paragraphs_num']]).reshape(-1, 1))[0]\n",
    "        item['words_len'] = words_len_encoder.transform(np.array([item['words_len']]).reshape(-1, 1))[0]\n",
    "        del item['pic_num']\n",
    "        item['source'] = source_encoder.transform(np.array([item['source']]).reshape(-1, 1)).toarray()[0]\n",
    "        \n",
    "#         del item['id']\n",
    "\n",
    "        if dataset_type == 'test':\n",
    "            item['doctype'] = -1\n",
    "        else:\n",
    "            item['doctype'] = doctype_list.index(item['doctype'])\n",
    "        label = item['doctype']\n",
    "        \n",
    "        del item['doctype']\n",
    "        \n",
    "        #         print(item)\n",
    "        \n",
    "        return item, label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_list[index]\n",
    "        item, label = MyDataset.get_data_label(self.tokenizer, item, self.dataset_type)\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def load_data(self, input_paths):\n",
    "        if type(input_paths) != list:\n",
    "            print(\"input_paths is not list!\")\n",
    "            return\n",
    "        data_list = []\n",
    "        for input_path in input_paths:\n",
    "            with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "                for line in tqdm(input_file):\n",
    "                    json_data = json.loads(line)\n",
    "                    data_list.append(json_data)\n",
    "        if self.dataset_type != 'test':\n",
    "            random.shuffle(data_list)\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = MyDataset([processed_expose_train_train_path], 'train')\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=True, num_workers=num_workers)\n",
    "# for batch_idx,data_ in enumerate(train_loader, 0):\n",
    "#     print(data_)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104282it [00:01, 87759.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['b6ff23b6-ea8d-4ba2-9b47-bef641f7cc52', '11e839b2-a98d-11eb-8239-7788095c0b0f'], 'category': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64), 'paragraphs_num': tensor([[-0.2993],\n",
      "        [-0.2387]], dtype=torch.float64), 'source': tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64), 'words_len': tensor([[-0.4038],\n",
      "        [-0.9272]], dtype=torch.float64), 'input_ids': tensor([[ 101.,  671., 3667.,  ...,  743., 1296.,  102.],\n",
      "        [ 101., 2425., 3926.,  ...,    0.,    0.,    0.]]), 'token_type_ids': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'attention_mask': tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])}\n",
      "tensor([0, 2])\n",
      "2\n",
      "tensor([0, 2])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset([processed_expose_train_train_path], 'train')\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "data_tmp = {'id':[],\n",
    "       'category':torch.tensor([]),\n",
    "       'paragraphs_num':torch.tensor([]),\n",
    "       'source':torch.tensor([]),\n",
    "       'words_len':torch.tensor([]),\n",
    "       'input_ids':torch.tensor([]),\n",
    "       'token_type_ids':torch.tensor([]),\n",
    "       'attention_mask':torch.tensor([]),}\n",
    "label_tmp = torch.tensor([], dtype=torch.int)\n",
    "count = 0\n",
    "for data_ in train_loader:\n",
    "#     print(data_)\n",
    "    data, label = data_\n",
    "#     print(label)\n",
    "    data_tmp['id'].extend(data['id'])\n",
    "    data_tmp['category'] = torch.cat((data_tmp['category'], data['category']), 0)\n",
    "    data_tmp['paragraphs_num'] = torch.cat((data_tmp['paragraphs_num'], data['paragraphs_num']), 0)\n",
    "    data_tmp['source'] = torch.cat((data_tmp['source'], data['source']), 0)\n",
    "    data_tmp['words_len'] = torch.cat((data_tmp['words_len'], data['words_len']), 0)\n",
    "    data_tmp['input_ids'] = torch.cat((data_tmp['input_ids'], data['input_ids']), 0)\n",
    "    data_tmp['token_type_ids'] = torch.cat((data_tmp['token_type_ids'], data['token_type_ids']), 0)\n",
    "    data_tmp['attention_mask'] = torch.cat((data_tmp['attention_mask'], data['attention_mask']), 0)\n",
    "    label_tmp = torch.cat((label_tmp, label), 0)\n",
    "    count += 1\n",
    "    if count > 1:\n",
    "        print(data_tmp)\n",
    "        print(label_tmp)\n",
    "        print(label_tmp.shape[0])\n",
    "        print(torch.where(label_tmp == 1, 2, label_tmp))\n",
    "        data_tmp = {'id':[],\n",
    "               'category':torch.tensor([]),\n",
    "               'paragraphs_num':torch.tensor([]),\n",
    "               'source':torch.tensor([]),\n",
    "               'words_len':torch.tensor([]),\n",
    "               'input_ids':torch.tensor([]),\n",
    "               'token_type_ids':torch.tensor([]),\n",
    "               'attention_mask':torch.tensor([]),}\n",
    "        label_tmp = torch.tensor([], dtype=torch.int64)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutilTargetModel(nn.Module):\n",
    "    \"\"\"Bert分类器模型\"\"\"\n",
    "    def __init__(self, hidden_size=768):\n",
    "        super(MutilTargetModel, self).__init__()\n",
    "        \n",
    "#         self.expert = AutoModel.from_pretrained(pretrained_bert_path)\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(pretrained_bert_path)\n",
    "        \n",
    "        category_size = len(category_encoder.categories_[0])\n",
    "        paragraphs_num_size = 1\n",
    "        words_len_size = 1\n",
    "        source_size = len(source_encoder.categories_[0])\n",
    "        expert_size = 10\n",
    "        linear_size = hidden_size + category_size + paragraphs_num_size + source_size + words_len_size + expert_size\n",
    "        self.all_net = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(linear_size, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            nn.Linear(64, len(doctype_list)),\n",
    "        )\n",
    "        \n",
    "        self.expert_net = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "            nn.Linear(64, expert_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        input_ids = batch_data['input_ids'].cuda()\n",
    "        token_type_ids = batch_data['token_type_ids'].cuda()\n",
    "        attention_mask = batch_data['attention_mask'].cuda()\n",
    "        \n",
    "        bert_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        bert_cls_hidden_state = bert_output[0][:, 0, :]\n",
    "        \n",
    "#         expert = self.expert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0][:, 0, :]\n",
    "        expert_output = self.expert_net(bert_cls_hidden_state)\n",
    "        \n",
    "        cat_layer = torch.cat((bert_cls_hidden_state, \n",
    "                               batch_data['category'].cuda(), \n",
    "                               batch_data['paragraphs_num'].cuda(), \n",
    "                               batch_data['source'].cuda(), \n",
    "                               batch_data['words_len'].cuda(),\n",
    "                               expert_output), \n",
    "                              1)\n",
    "#         print(cat_layer.shape)\n",
    "        all_output = self.all_net(cat_layer.to(torch.float32))\n",
    "\n",
    "        return all_output, expert_output\n",
    "    \n",
    "    def freeze(self, layer):\n",
    "        for child in layer.children():\n",
    "#             print(child)\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "#                 print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uZ11cKkEBgwD"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}. score:{-score}')\n",
    "            with open(\"early_stop.log\", 'a+', encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f'{localtime} EarlyStopping counter: {self.counter} out of {self.patience}. score:{-score}\\n')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            localtime = time.asctime( time.localtime(time.time()) )\n",
    "            with open(\"early_stop.log\", 'a+', encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f'{localtime} Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\\n')\n",
    "#         torch.save(model.state_dict(), model_train_mutil_model_path)\t# 这里会存储迄今最优模型的参数\n",
    "        torch.save(model, model_train_mutil_model_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9_I2GYjXoXr",
    "outputId": "1dacd6af-3f6c-431e-9c62-7dcbc45c8145"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "model = MutilTargetModel()\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "model = model.cuda()\n",
    "# print(model)\n",
    "\n",
    "# 不同子网络设定不同的学习率\n",
    "Bert_model_param = []\n",
    "Bert_downstream_param = []\n",
    "for items, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "#         print(items)\n",
    "        continue\n",
    "    if items.startswith('expert.') or items.startswith('expert_net.'):\n",
    "        continue\n",
    "    if items.startswith('bert.'):\n",
    "        Bert_model_param.append(param)\n",
    "    else:\n",
    "        Bert_downstream_param.append(param)\n",
    "param_groups = [{\"params\": Bert_model_param, \"lr\": 1e-5},\n",
    "                {\"params\": Bert_downstream_param, \"lr\": 1e-4}]\n",
    "all_optimizer = optim.Adam(param_groups, eps=1e-7, weight_decay=0.001)\n",
    "\n",
    "Bert_model_param = []\n",
    "Bert_downstream_param = []\n",
    "for items, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "#         print(items)\n",
    "        continue\n",
    "    if not items.startswith('expert.') or items.startswith('expert_net.'):\n",
    "        continue\n",
    "    if items.startswith('bert.'):\n",
    "        Bert_model_param.append(param)\n",
    "    else:\n",
    "        Bert_downstream_param.append(param)\n",
    "param_groups = [{\"params\": Bert_model_param, \"lr\": 1e-5},\n",
    "                {\"params\": Bert_downstream_param, \"lr\": 1e-4}]\n",
    "expert_optimizer = optim.Adam(param_groups, eps=1e-7, weight_decay=0.001)\n",
    "# 初始化 early_stopping 对象\n",
    "patience = 2\t# 当验证集损失在连续n次训练周期中都没有得到降低时，停止模型训练，以防止模型过拟合\n",
    "early_stopping = EarlyStopping(patience, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpBMPyqEXxJb",
    "outputId": "7ae2bcc6-1d47-4c69-ba8f-9be9d53efb52"
   },
   "outputs": [],
   "source": [
    "# epoch_num = 5\n",
    "# batch_loss_num = 100\n",
    "# for epoch in range(epoch_num):\n",
    "#     model.train()\t# 设置模型为训练模式\n",
    "#     train_dataset = MyDataset([processed_expose_train_train_path], 'train')\n",
    "#     train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#     with tqdm(total=len(train_loader)) as t:\n",
    "#         loss_sum = 0.0\n",
    "#         batch_loss = 1.0\n",
    "#         for batch_idx,data_ in enumerate(train_loader, 0):\n",
    "#             data, label = data_\n",
    "#             # 清空梯度\n",
    "#             all_optimizer.zero_grad()\n",
    "#             expert_optimizer.zero_grad()\n",
    "            \n",
    "#             all_output, expert_output = model(data)\n",
    "            \n",
    "#             all_loss = criterion(all_output, label.cuda())\n",
    "# #             all_loss = all_loss / 2\n",
    "            \n",
    "# #             print(label)\n",
    "# #             print(expert_output)\n",
    "# #             print(all_output)\n",
    "# #             expert_predict = np.argmax(expert_output.cpu(), 1)\n",
    "#             other_index = doctype_list.index('其他')\n",
    "#             expert_predict = torch.tensor(np.argmax(expert_output.cpu().detach().numpy(), 1))\n",
    "# #             print(expert_predict)\n",
    "#             expert_label = torch.where(label > other_index, label - 1, label)\n",
    "# #             print(expert_label)\n",
    "#             expert_label = torch.where(label == other_index, expert_predict, expert_label)\n",
    "# #             expert_label = label.clone().detach().float().requires_grad_()\n",
    "# #             for i in range(label.shape[0]):\n",
    "# #                 if label[i] == doctype_list.index('其他'):\n",
    "# #                     expert_label[i] = torch.max(expert_output[i])\n",
    "# #             print(expert_label)\n",
    "# #             expert_loss = criterion(expert_output, expert_label.cuda())\n",
    "#             expert_loss = criterion(expert_output, expert_label.cuda())\n",
    "# #             expert_loss = expert_loss / 2\n",
    "    \n",
    "#             all_loss.backward(retain_graph=True)\n",
    "#             expert_loss.backward()\n",
    "#             # 更新模型参数\n",
    "#             all_optimizer.step()\n",
    "#             expert_optimizer.step()\n",
    "            \n",
    "#             loss_sum += all_loss.item()\n",
    "#             if batch_idx % batch_loss_num == batch_loss_num - 1:\n",
    "#                 batch_loss = loss_sum / batch_loss_num\n",
    "#                 localtime = time.asctime( time.localtime(time.time()) )\n",
    "#                 with open(\"early_stop.log\", 'a+', encoding=\"utf-8\") as log_file:\n",
    "#                     log_file.write(f'{localtime} train_loss={all_loss.item():.6f} batch_loss={batch_loss:.6f}\\n')\n",
    "#                 loss_sum = 0.0\n",
    "          \n",
    "#             t.set_postfix_str(f'train_loss={all_loss.item():.6f} batch_loss={batch_loss:.6f}')\n",
    "#             t.update()\n",
    "            \n",
    "#             del data, label, all_output, expert_output \n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "#     #----------------------------------------------------\n",
    "#     model.eval() # 设置模型为评估/测试模式\n",
    "#     valid_dataset = MyDataset([processed_expose_train_valid_path], 'valid')\n",
    "#     valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#     valid_loss_sum = 0.0\n",
    "#     with tqdm(total=len(valid_loader)) as t:\n",
    "#         with torch.no_grad():\n",
    "#             for data, label in valid_loader:\n",
    "#                 # 一般如果验证集不是很大的话，模型验证就不需要按批量进行了，但要注意输入参数的维度不能错\n",
    "#                 all_output, _ = model(data)\n",
    "#                 all_loss = criterion(all_output, label.cuda())\n",
    "#                 valid_loss_sum += all_loss.item()\n",
    "#                 t.set_postfix_str(f'valid loss={all_loss.item()}')\n",
    "#                 t.update()\n",
    "#     early_stopping(valid_loss_sum, model)\n",
    "#     # 若满足 early stopping 要求\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         # 结束模型训练\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存完整的 BERT 分类器模型\n",
    "# torch.save(model, model_train_mutil_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0eA7HdOqIwZe"
   },
   "outputs": [],
   "source": [
    "# 获得 early stopping 时的模型参数\n",
    "# model.load_state_dict(torch.load(model_train_mutil_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76454it [00:00, 83537.48it/s]\n",
      "39347it [00:00, 95480.95it/s]\n",
      "  0%|                                    | 37/57901 [00:19<8:14:00,  1.95it/s, train_loss=2.388212 batch_loss=1.000000]"
     ]
    }
   ],
   "source": [
    "epoch_num = 1\n",
    "batch_loss_num = 100\n",
    "for epoch in range(epoch_num):\n",
    "    model.train()\t# 设置模型为训练模式\n",
    "    train_dataset = MyDataset([processed_expose_train_labeled_path, processed_expose_train_unlabel_predict_other_path], 'train')\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    with tqdm(total=len(train_loader)) as t:\n",
    "        loss_sum = 0.0\n",
    "        batch_loss = 1.0\n",
    "        for batch_idx,data_ in enumerate(train_loader, 0):\n",
    "            data, label = data_\n",
    "             # 清空梯度\n",
    "            all_optimizer.zero_grad()\n",
    "            expert_optimizer.zero_grad()\n",
    "            \n",
    "            all_output, expert_output = model(data)\n",
    "            \n",
    "            all_loss = criterion(all_output, label.cuda())\n",
    "#             all_loss = all_loss / 2\n",
    "            \n",
    "            other_index = doctype_list.index('其他')\n",
    "#             print(expert_output)\n",
    "            expert_predict = torch.tensor(np.argmax(expert_output.cpu().detach().numpy(), 1))\n",
    "#             print(expert_predict)\n",
    "#             print(label)\n",
    "#             expert_label = torch.where(label > other_index, label - 1, label)\n",
    "#             expert_label = torch.where(label == other_index, expert_predict, expert_label)\n",
    "            expert_label = label.clone().detach()\n",
    "            for i in range(label.shape[0]):\n",
    "                if label[i] == other_index:\n",
    "                    expert_label[i] = expert_predict[i]\n",
    "                elif label[i] > other_index:\n",
    "                    expert_label[i] -= 1\n",
    "#             print(expert_label)\n",
    "            expert_loss = criterion(expert_output, expert_label.cuda())\n",
    "#             expert_loss = expert_loss / 2\n",
    "    \n",
    "            all_loss.backward(retain_graph=True)\n",
    "            expert_loss.backward()\n",
    "            # 更新模型参数\n",
    "            all_optimizer.step()\n",
    "            expert_optimizer.step()\n",
    "            \n",
    "            loss_sum += all_loss.item()\n",
    "            if batch_idx % batch_loss_num == batch_loss_num - 1:\n",
    "                batch_loss = loss_sum / batch_loss_num\n",
    "                localtime = time.asctime( time.localtime(time.time()) )\n",
    "                with open(\"early_stop.log\", 'a+', encoding=\"utf-8\") as log_file:\n",
    "                    log_file.write(f'{localtime} train_loss={all_loss.item():.6f} batch_loss={batch_loss:.6f}\\n')\n",
    "                loss_sum = 0.0\n",
    "          \n",
    "            t.set_postfix_str(f'train_loss={all_loss.item():.6f} batch_loss={batch_loss:.6f}')\n",
    "            t.update()\n",
    "            \n",
    "            del data, label, all_output, expert_output \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 保存完整的 BERT 分类器模型\n",
    "    torch.save(model, model_train_mutil_model_path)\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    with open(\"early_stop.log\", 'a+', encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(f'{localtime} Saving model ...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(model_train_mutil_model_path)\n",
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 设置模型为评估/测试模式\n",
    "valid_dataset = MyDataset([processed_expose_train_valid_path], 'valid')\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(valid_loader):\n",
    "        print(data, label)\n",
    "        print(data['id'][0])\n",
    "        output, _ = model(data)\n",
    "#         print(output)\n",
    "#         print(torch.sigmoid(output))\n",
    "#         sigmoid = nn.Sigmoid()\n",
    "#         print(sigmoid(output))\n",
    "        predict_list = F.softmax(output, dim=1).tolist()[0]\n",
    "        predict_index = np.argmax(predict_list)\n",
    "        predict_label = doctype_list[predict_index]\n",
    "        print(predict_label)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/processed_train_expose_labeled.json\", \"r\", encoding='utf-8') as input_file:\n",
    "#     for line in tqdm(input_file):\n",
    "#         json_data = json.loads(line)\n",
    "#         if json_data['id'] == 'ee137ac3-c2a2-4aba-a517-36840ffd2f1a':\n",
    "#             print(line)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval() # 设置模型为评估/测试模式\n",
    "# valid_dataset = MyDataset(\"data/test.txt\", 'test')\n",
    "# valid_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "# with torch.no_grad():\n",
    "#     for data, label in tqdm(valid_loader):\n",
    "# #         print(data, label)\n",
    "#         output = model(data)\n",
    "# #         print(output)\n",
    "# #         print(torch.sigmoid(output))\n",
    "#         print(F.softmax(output, dim=1).tolist()[0])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 设置模型为评估/测试模式\n",
    "test_dataset = MyDataset([processed_expose_test_path], 'test')\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "predict_doctype = {}\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(test_loader):\n",
    "        id = data['id'][0]\n",
    "        output, _ = model(data)\n",
    "        predict_list = F.softmax(output, dim=1).tolist()[0]\n",
    "        predict_index = np.argmax(predict_list)\n",
    "        predict_label = doctype_list[predict_index]\n",
    "        predict_doctype[id] = predict_label\n",
    "predict_data = {'predict_doctype' : predict_doctype}\n",
    "df = pd.DataFrame(predict_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name = 'id'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission_train_mutil_target_predict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看起来，复杂、大模型是有效的(在验证数据集上，第二个epoch的loss还在降低)，所以下一步准备试一下cnn和albert(或roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2021_huawei_ai_match.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
