{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5g1l-MySv13",
    "outputId": "717a5728-a74a-44f1-d47d-c0510945e390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 19 22:53:50 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 457.49       Driver Version: 457.49       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   39C    P8     6W /  N/A |   1311MiB /  6144MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1684    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      3148    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      7584    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      9156    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10212    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     10592    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     12980    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     13528    C+G   ...sk\\baidunetdiskrender.exe    N/A      |\n",
      "|    0   N/A  N/A     15380    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A     15960    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     16052    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     38788    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     44596    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     46780    C+G   D:\\Battle.net\\Battle.net.exe    N/A      |\n",
      "|    0   N/A  N/A     49704    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     50844    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     56660    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     58572    C+G   ...tracted\\WechatBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A     60800    C+G   ...4\\extracted\\WeChatApp.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UetA7O1OU3ju"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nlKDWXwdVIIM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7sb1mTUJVLYz"
   },
   "outputs": [],
   "source": [
    "processed_expose_test_path = \"data/processed_test_expose.json\"\n",
    "processed_expose_train_labeled_path = \"data/processed_train_expose_labeled.json\"\n",
    "processed_expose_train_labeled_train_path = \"data/processed_train_expose_labeled_train.json\"\n",
    "processed_expose_train_labeled_valid_path = \"data/processed_train_expose_labeled_valid.json\"\n",
    "processed_expose_train_unlabel_path = \"data/processed_train_expose_unlabel.json\"\n",
    "processed_expose_train_unlabel_predict_list_path = \"data/processed_train_expose_unlabel_predict_list.json\"\n",
    "\n",
    "pretrained_bert_path = \"bert-base-chinese/\"\n",
    "\n",
    "category_encoder_path = 'model/category_encoder.pickle'\n",
    "paragraphs_num_encoder_path = 'model/paragraphs_num_encoder.pickle'\n",
    "source_encoder_path = 'model/source_encoder.pickle'\n",
    "doctype_encoder_path = 'model/doctype_encoder.pickle'\n",
    "words_len_encoder_path = 'model/words_len_encoder.pickle'\n",
    "model_train_first_path = 'model/model_train_first.pt'\n",
    "\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(processed_expose_train_path, 'r', encoding=\"utf-8\") as input_file, \\\n",
    "#      open(processed_expose_train_labeled_path, 'w', encoding=\"utf-8\") as labeled_file, \\\n",
    "#      open(processed_expose_train_unlabel_path, 'w', encoding=\"utf-8\") as unlabel_file:\n",
    "#     for line in tqdm(input_file):\n",
    "#         json_data = json.loads(line)\n",
    "#         if json_data['doctype'] != '':\n",
    "#             labeled_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")\n",
    "#         else:\n",
    "#             unlabel_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xvyXmwjiVwld"
   },
   "outputs": [],
   "source": [
    "def get_feature_list(input_path, feature_name):\n",
    "    feature_list = []\n",
    "    with open(input_path, 'r', encoding=\"utf-8\") as input_file:\n",
    "        for line in tqdm(input_file):\n",
    "            json_data = json.loads(line)\n",
    "            feature_list.append(json_data[feature_name])\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0jZkH8nNVzKW"
   },
   "outputs": [],
   "source": [
    "def get_category_encoder():\n",
    "    if os.path.exists(category_encoder_path):\n",
    "        with open (category_encoder_path, 'rb') as category_encoder_file: \n",
    "            return pickle.load(category_encoder_file)\n",
    "    \n",
    "    category_list = get_feature_list(processed_expose_train_labeled_path, 'category') + get_feature_list(processed_expose_test_path, 'category')\n",
    "    category_list = np.array(category_list).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore').fit(category_list)\n",
    "    \n",
    "    with open (category_encoder_path, 'wb') as category_encoder_file:\n",
    "        pickle.dump(encoder, category_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xgSN3_bV2Oo",
    "outputId": "6669d666-e358-4831-987e-fcd26dabe8ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_encoder = get_category_encoder()\n",
    "category_encoder.transform(np.array([1]).reshape(-1, 1)).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDMalpnWV54s",
    "outputId": "0346b08b-9962-4b7e-b5cc-f2e8e8006389"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "obSOBSJgV9HM"
   },
   "outputs": [],
   "source": [
    "def get_paragraphs_num_encoder():\n",
    "    if os.path.exists(paragraphs_num_encoder_path):\n",
    "        with open (paragraphs_num_encoder_path, 'rb') as paragraphs_num_encoder_file: \n",
    "            return pickle.load(paragraphs_num_encoder_file)\n",
    "    \n",
    "    paragraphs_num_list = get_feature_list(processed_expose_train_labeled_path, 'paragraphs_num') + get_feature_list(processed_expose_test_path, 'paragraphs_num')\n",
    "    paragraphs_num_list = np.array(paragraphs_num_list).reshape(-1, 1)\n",
    "    encoder = StandardScaler().fit(paragraphs_num_list)\n",
    "    \n",
    "    with open (paragraphs_num_encoder_path, 'wb') as paragraphs_num_encoder_file:\n",
    "        pickle.dump(encoder, paragraphs_num_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J0gKWqDV_Hy",
    "outputId": "38441f14-d57d-4e9c-b637-d7adf35c663e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.3295779947518716"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_num_encoder = get_paragraphs_num_encoder()\n",
    "paragraphs_num_encoder.transform(np.array([100]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KAcTDhsWB_p",
    "outputId": "93b675f1-a6d7-4de4-e49e-1e590394b713"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5684543631.472906"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_num_encoder.transform(np.array([99999999999]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3wN3dvYs_cj3"
   },
   "outputs": [],
   "source": [
    "def get_words_len_encoder():\n",
    "    if os.path.exists(words_len_encoder_path):\n",
    "        with open (words_len_encoder_path, 'rb') as words_len_encoder_file: \n",
    "            return pickle.load(words_len_encoder_file)\n",
    "    \n",
    "    words_len_list = get_feature_list(processed_expose_train_labeled_path, 'words_len') + get_feature_list(processed_expose_test_path, 'words_len')\n",
    "    words_len_list = np.array(words_len_list).reshape(-1, 1)\n",
    "    encoder = StandardScaler().fit(words_len_list)\n",
    "    \n",
    "    with open (words_len_encoder_path, 'wb') as words_len_encoder_file:\n",
    "        pickle.dump(encoder, words_len_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5s7Mmgd_gVA",
    "outputId": "9d59c3cd-19fe-4f54-abb3-025f4b8517e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5472610685490511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_len_encoder = get_words_len_encoder()\n",
    "words_len_encoder.transform(np.array([1000]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RO6ZkBX_kLH",
    "outputId": "de4535dc-dd1a-47c5-9f81-e52cb0bc4907"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2779654221391926"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_len_encoder.transform(np.array([2000]).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uW0geYy1WEmb"
   },
   "outputs": [],
   "source": [
    "def get_source_encoder():\n",
    "    if os.path.exists(source_encoder_path):\n",
    "        with open (source_encoder_path, 'rb') as source_encoder_file: \n",
    "            return pickle.load(source_encoder_file)\n",
    "    \n",
    "    source_list = get_feature_list(processed_expose_train_labeled_path, 'source') + get_feature_list(processed_expose_test_path, 'source')\n",
    "    source_list = np.array(source_list).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore').fit(source_list)\n",
    "    \n",
    "    with open (source_encoder_path, 'wb') as source_encoder_file:\n",
    "        pickle.dump(encoder, source_encoder_file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fD1CoDpiWHDf",
    "outputId": "6e4eddef-9bcb-4d70-bf6a-6c7d5ece2803"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_encoder = get_source_encoder()\n",
    "source_encoder.transform(np.array(['中国经济周刊']).reshape(-1, 1)).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rSifyLaWKd5",
    "outputId": "3c088af8-e2a8-46bc-c32d-472e5c34474a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_encoder.transform(np.array(['hg']).reshape(-1, 1)).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpW8Ia_SWNhL",
    "outputId": "b3cbd0bd-fefc-478e-b102-d0113c0c2544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2b99VHK1WP-T"
   },
   "outputs": [],
   "source": [
    "def get_doctype_encoder():\n",
    "    if os.path.exists(doctype_encoder_path):\n",
    "        with open (doctype_encoder_path, 'rb') as doctype_encoder_file: \n",
    "            return pickle.load(doctype_encoder_file)\n",
    "    \n",
    "    doctype_list = get_feature_list(processed_expose_train_labeled_path, 'doctype')\n",
    "    doctype_set = set(doctype_list)\n",
    "    doctype_list = list(doctype_set)\n",
    "    \n",
    "    with open (doctype_encoder_path, 'wb') as doctype_encoder_file:\n",
    "        pickle.dump(doctype_list, doctype_encoder_file)\n",
    "    return doctype_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKsMBeGYWSXT",
    "outputId": "14f57c56-76ec-43ac-ae66-6419e3917149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['作品分析',\n",
       "  '治愈系文章',\n",
       "  '情感解读',\n",
       "  '行业解读',\n",
       "  '科普知识文',\n",
       "  '深度事件',\n",
       "  '推荐文',\n",
       "  '攻略文',\n",
       "  '人物专栏',\n",
       "  '物品评测'],\n",
       " 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctype_list = get_doctype_encoder()\n",
    "doctype_list, len(doctype_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxxfHEhE_vvN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aqRPWxxo_vzh"
   },
   "outputs": [],
   "source": [
    "# with open(processed_expose_train_labeled_path, 'r', encoding=\"utf-8\") as input_file, \\\n",
    "#      open(processed_expose_train_labeled_train_path, 'w', encoding=\"utf-8\") as train_file, \\\n",
    "#      open(processed_expose_train_labeled_valid_path, 'w', encoding=\"utf-8\") as valid_file:\n",
    "#     for line in tqdm(input_file):\n",
    "#         json_data = json.loads(line)\n",
    "#         if random.random() < 0.1:\n",
    "#             valid_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")\n",
    "#         else:\n",
    "#             train_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdkeBfBeWYLR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7FIdsWBCWYPi"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, input_path, dataset_type):\n",
    "        self.dataset_type = dataset_type\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_path)\n",
    "        self.data_list = self.load_data(input_path)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_data_label(cls, tokenizer, item, dataset_type):\n",
    "        token = tokenizer(item['text'], add_special_tokens=True,\n",
    "                                              max_length=512,\n",
    "                                              truncation=True,\n",
    "                                              padding='max_length',\n",
    "                                              return_tensors=\"pt\")\n",
    "        del item['text']\n",
    "#         item = dict(item, **token)\n",
    "        item['input_ids'] = token['input_ids'][0]\n",
    "        item['token_type_ids'] = token['token_type_ids'][0]\n",
    "        item['attention_mask'] = token['attention_mask'][0]\n",
    "        \n",
    "        item['category'] = category_encoder.transform(np.array([item['category']]).reshape(-1, 1)).toarray()[0]\n",
    "        item['paragraphs_num'] = paragraphs_num_encoder.transform(np.array([item['paragraphs_num']]).reshape(-1, 1))[0]\n",
    "        item['words_len'] = words_len_encoder.transform(np.array([item['words_len']]).reshape(-1, 1))[0]\n",
    "        del item['pic_num']\n",
    "        item['source'] = source_encoder.transform(np.array([item['source']]).reshape(-1, 1)).toarray()[0]\n",
    "        \n",
    "        del item['id']\n",
    "\n",
    "        if dataset_type == 'test':\n",
    "            item['doctype'] = -1\n",
    "        else:\n",
    "            item['doctype'] = doctype_list.index(item['doctype'])\n",
    "        label = item['doctype']\n",
    "        \n",
    "        del item['doctype']\n",
    "        \n",
    "        #         print(item)\n",
    "        \n",
    "        return item, label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_list[index]\n",
    "        item, label = MyDataset.get_data_label(self.tokenizer, item, self.dataset_type)\n",
    "        return item, label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def load_data(self, input_path):\n",
    "        data_list = []\n",
    "        with open(input_path, 'r', encoding='utf-8') as input_file:\n",
    "            for line in tqdm(input_file):\n",
    "                json_data = json.loads(line)\n",
    "                data_list.append(json_data)\n",
    "        if self.dataset_type != 'test':\n",
    "            random.shuffle(data_list)\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-LpsS3-gWbkm"
   },
   "outputs": [],
   "source": [
    "# train_dataset = MyDataset(processed_expose_train_labeled_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Eh_Gw3ZjWiZs"
   },
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=True, num_workers=num_workers)\n",
    "# for data, label in tqdm(train_loader):\n",
    "#     print(data)\n",
    "#     print(label)\n",
    "\n",
    "# #     bert_output = bert(input_ids=data['input_ids'], token_type_ids=data['token_type_ids'], attention_mask=data['attention_mask'])\n",
    "# #     print(bert_output)\n",
    "# #     bert_cls_hidden_state = bert_output[0][:, 0, :]\n",
    "# #     print(bert_cls_hidden_state)\n",
    "# #     print(bert_cls_hidden_state.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SUofHcMZW7DN"
   },
   "outputs": [],
   "source": [
    "class BertClassificationModel(nn.Module):\n",
    "    \"\"\"Bert分类器模型\"\"\"\n",
    "    def __init__(self, hidden_size=768):\n",
    "        super(BertClassificationModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_bert_path)\n",
    "        \n",
    "        category_size = len(category_encoder.categories_[0])\n",
    "        paragraphs_num_size = 1\n",
    "        words_len_size = 1\n",
    "        source_size = len(source_encoder.categories_[0])\n",
    "        linear_size = hidden_size + category_size + paragraphs_num_size + source_size + words_len_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(linear_size, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.out = nn.Linear(64, len(doctype_list))\n",
    "#         self.out = nn.Linear(linear_size, len(doctype_list))\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        input_ids = batch_data['input_ids'].clone().detach().cuda()\n",
    "        token_type_ids = batch_data['token_type_ids'].clone().detach().cuda()\n",
    "        attention_mask = batch_data['attention_mask'].clone().detach().cuda()\n",
    "        bert_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        bert_cls_hidden_state = bert_output[0][:, 0, :]\n",
    "#         print(bert_cls_hidden_state.shape)\n",
    "#         print(batch_data['category'].shape)\n",
    "#         print(batch_data['paragraphs_num'].shape)\n",
    "#         print(batch_data['source'].shape)\n",
    "        category = batch_data['category'].clone().detach().cuda()\n",
    "        paragraphs_num = batch_data['paragraphs_num'].clone().detach().cuda()\n",
    "        source = batch_data['source'].clone().detach().cuda()\n",
    "        words_len = batch_data['words_len'].clone().detach().cuda()\n",
    "        cat_layer = torch.cat((bert_cls_hidden_state, category, paragraphs_num, source, words_len), 1)\n",
    "#         print(cat_layer.shape)\n",
    "        output = self.net(cat_layer.to(torch.float32))\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ11cKkEBgwD"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "#         torch.save(model.state_dict(), model_train_first_path)\t# 这里会存储迄今最优模型的参数\n",
    "        torch.save(model, model_train_first_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9_I2GYjXoXr",
    "outputId": "1dacd6af-3f6c-431e-9c62-7dcbc45c8145"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "model = BertClassificationModel()\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "model = model.cuda()\n",
    "# print(model)\n",
    "\n",
    "# 不同子网络设定不同的学习率\n",
    "Bert_model_param = []\n",
    "Bert_downstream_param = []\n",
    "for items, _ in model.named_parameters():\n",
    "    if \"bert\" in items:\n",
    "        Bert_model_param.append(_)\n",
    "    else:\n",
    "        Bert_downstream_param.append(_)\n",
    "param_groups = [{\"params\": Bert_model_param, \"lr\": 1e-5},\n",
    "                {\"params\": Bert_downstream_param, \"lr\": 1e-4}]\n",
    "optimizer = optim.Adam(param_groups, eps=1e-7, weight_decay=0.001)\n",
    "# 初始化 early_stopping 对象\n",
    "patience = 2\t# 当验证集损失在连续n次训练周期中都没有得到降低时，停止模型训练，以防止模型过拟合\n",
    "early_stopping = EarlyStopping(patience, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpBMPyqEXxJb",
    "outputId": "7ae2bcc6-1d47-4c69-ba8f-9be9d53efb52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68920it [00:00, 85117.89it/s]\n",
      "100%|█████████████████████████████████| 34460/34460 [4:32:10<00:00,  2.11it/s, train_loss=0.701839 batch_loss=0.609479]\n",
      "7534it [00:00, 89649.39it/s]\n",
      "100%|███████████████████████████████████████████████| 3767/3767 [04:34<00:00, 13.73it/s, valid loss=0.1909613162279129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 2116.414926).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68920it [00:00, 87001.94it/s]\n",
      "100%|█████████████████████████████████| 34460/34460 [4:32:39<00:00,  2.11it/s, train_loss=0.414682 batch_loss=0.635362]\n",
      "7534it [00:00, 84626.53it/s]\n",
      "100%|███████████████████████████████████████████████| 3767/3767 [04:37<00:00, 13.57it/s, valid loss=0.3082568645477295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68920it [00:00, 82222.99it/s]\n",
      " 23%|███████▋                          | 7833/34460 [54:10<3:12:27,  2.31it/s, train_loss=0.028321 batch_loss=0.701261]"
     ]
    }
   ],
   "source": [
    "# epoch_num = 5\n",
    "# batch_loss_num = 100\n",
    "# for epoch in range(epoch_num):\n",
    "#     model.train()\t# 设置模型为训练模式\n",
    "#     train_dataset = MyDataset(processed_expose_train_labeled_train_path, 'train')\n",
    "#     train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#     with tqdm(total=len(train_loader)) as t:\n",
    "#         loss_sum = 0.0\n",
    "#         batch_loss = 1.0\n",
    "#         for batch_idx,data_ in enumerate(train_loader, 0):\n",
    "#             data, label = data_\n",
    "#             # 清空梯度\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, label.cuda())\n",
    "# #             output = model(data)\n",
    "# #             loss = criterion(output, label)\n",
    "#             loss.backward()\n",
    "\n",
    "#             # 更新模型参数\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             loss_sum += loss.item()\n",
    "#             if batch_idx % batch_loss_num == batch_loss_num - 1:\n",
    "#                 batch_loss = loss_sum / batch_loss_num\n",
    "#                 loss_sum = 0.0\n",
    "          \n",
    "#             t.set_postfix_str(f'train_loss={loss.item():.6f} batch_loss={batch_loss:.6f}')\n",
    "#             t.update()\n",
    "            \n",
    "#             del data, label, output \n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "#     #----------------------------------------------------\n",
    "#     model.eval() # 设置模型为评估/测试模式\n",
    "#     valid_dataset = MyDataset(processed_expose_train_labeled_valid_path, 'valid')\n",
    "#     valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#     valid_loss_sum = 0.0\n",
    "#     with tqdm(total=len(valid_loader)) as t:\n",
    "#         with torch.no_grad():\n",
    "#             for data, label in valid_loader:\n",
    "#                 # 一般如果验证集不是很大的话，模型验证就不需要按批量进行了，但要注意输入参数的维度不能错\n",
    "#                 output = model(data)\n",
    "#                 loss = criterion(output, label.cuda())\n",
    "# #                 output = model(data)\n",
    "# #                 loss = criterion(output, label)\n",
    "#                 valid_loss_sum += loss.item()\n",
    "#                 t.set_postfix_str(f'valid loss={loss.item()}')\n",
    "#                 t.update()\n",
    "#     early_stopping(valid_loss_sum, model)\n",
    "#     # 若满足 early stopping 要求\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         # 结束模型训练\n",
    "#         break\n",
    "#     # # 保存完整的 BERT 分类器模型\n",
    "#     # torch.save(model, model_train_first_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, model_train_first_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eA7HdOqIwZe"
   },
   "outputs": [],
   "source": [
    "# 获得 early stopping 时的模型参数\n",
    "# model.load_state_dict(torch.load(model_train_first_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76454it [00:00, 88671.66it/s]\n",
      "100%|█████████████████████████████████| 38227/38227 [5:28:40<00:00,  1.94it/s, train_loss=3.117380 batch_loss=0.535170]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 1\n",
    "batch_loss_num = 100\n",
    "for epoch in range(epoch_num):\n",
    "    model.train()\t# 设置模型为训练模式\n",
    "    train_dataset = MyDataset(processed_expose_train_labeled_path, 'train')\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    with tqdm(total=len(train_loader)) as t:\n",
    "        loss_sum = 0.0\n",
    "        batch_loss = 1.0\n",
    "        for batch_idx,data_ in enumerate(train_loader, 0):\n",
    "            data, label = data_\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label.cuda())\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # 更新模型参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "            if batch_idx % batch_loss_num == batch_loss_num - 1:\n",
    "                batch_loss = loss_sum / batch_loss_num\n",
    "                loss_sum = 0.0\n",
    "          \n",
    "            t.set_postfix_str(f'train_loss={loss.item():.6f} batch_loss={batch_loss:.6f}')\n",
    "            t.update()\n",
    "            \n",
    "            del data, label, output \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 保存完整的 BERT 分类器模型\n",
    "    torch.save(model, model_train_first_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_train_first_path)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7534it [00:00, 85588.30it/s]\n",
      "  0%|                                                                                         | 0/7534 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['c3fb4257-64ed-4c43-bc13-344d890cf229'], 'category': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64), 'paragraphs_num': tensor([[-0.2981]], dtype=torch.float64), 'source': tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64), 'words_len': tensor([[-0.5431]], dtype=torch.float64), 'input_ids': tensor([[ 101,  679, 2586, 1762, 5632, 2346, 4638, 3457, 2682, 7027, 6649,  948,\n",
      "         8024, 1372, 2586, 1762, 1166,  782, 4638, 1936, 6839,  704, 6837, 6662,\n",
      "          511, 4495, 3833, 3766, 3300, 3719, 6823, 4638, 2768, 1216, 8024, 1372,\n",
      "         3300, 1762, 2919, 2835,  704, 4991, 6629, 2798, 3221, 4696, 3633, 4638,\n",
      "         2768, 1216, 8024, 1372, 3300, 7306, 1045, 4638,  782, 4495, 2798, 5050,\n",
      "         3221, 4495, 1462, 4638, 3719, 2608,  511,  862, 3198,  952,  872, 6963,\n",
      "         1377,  809, 2458, 1993,  976, 5632, 2346, 2682,  976, 4638,  752, 8024,\n",
      "         1372, 6206,  872,  679, 4500, 2399, 7977, 1469, 1071,  800,  691, 6205,\n",
      "         1343, 3338, 5359, 5632, 2346, 8024, 3680,  702,  782, 2552,  704, 6963,\n",
      "         3300,  671, 4275, 3862, 8024, 5632, 2346,  679, 2813, 2359, 8024, 3766,\n",
      "          782, 2376,  872, 1423, 5661, 8024, 1222, 1213, 8024, 2218, 5543, 6878,\n",
      "         6224, 3291, 1962, 4638, 5632, 2346,  511, 3146, 4415,  671,  678, 5632,\n",
      "         2346, 4638, 2552, 2658, 8024, 2563, 6381, 6929,  763,  679, 2690, 2571,\n",
      "         4638, 2518,  752, 8024, 1420, 1420, 7509,  727, 8024, 4692, 4692, 7599,\n",
      "         3250, 8024, 6432, 5543, 6432, 4638, 6413, 8024,  976, 1377,  976, 4638,\n",
      "          752, 8024, 6624, 6421, 6624, 4638, 6662, 8024, 6224, 2682, 6224, 4638,\n",
      "          782,  511, 3123,  678,  872, 4638, 2750, 2680, 8024, 6421, 1962, 1962,\n",
      "         1222, 1213,  749,  511, 4495, 3833, 3766, 3300,  671, 2359, 7599, 7556,\n",
      "         4638, 8024, 1372, 3300, 2847, 5769, 3168, 3475, 2798, 5543, 6662, 6662,\n",
      "         7556,  511, 3123,  678,  872, 4638, 6576, 2046, 8024, 3300, 1927, 2553,\n",
      "         3300, 2533,  511, 3123,  678,  872, 4638, 5994, 5783, 8024, 1166, 5632,\n",
      "          809,  711, 3221,  511, 4495, 3833, 3766, 3300, 4696, 3633, 4638, 2130,\n",
      "         5401, 8024, 1372, 3300,  679, 2130, 5401, 2798, 3221, 3297, 4696, 2141,\n",
      "         4638, 5401,  511, 3140,  754, 7481, 2190, 1737, 1862, 4638,  782, 8024,\n",
      "         4495, 1462, 1728, 3634, 1780, 2487,  511, 3123,  678,  872, 4638, 2857,\n",
      "         2569, 8024, 1235, 3140, 6624, 5632, 2346, 4638, 6662,  511,  976,  702,\n",
      "         6121, 1220, 3836, 1416, 8024, 3680,  671,  702, 2544, 2207, 4638, 3121,\n",
      "         1359, 8024, 2772, 6387, 2218,  833, 5314,  872, 4638,  782, 4495, 2372,\n",
      "         3341, 1920,  679, 1398,  511, 1222, 1213, 3297, 1920, 4638, 2692,  721,\n",
      "         8038, 1762,  754, 6450, 3724, 3291, 1914, 4638, 6848, 2885, 3326, 8024,\n",
      "          996, 5898, 3291, 1914, 4638, 2128, 1059, 2697, 8024, 6375, 1079, 2552,\n",
      "          679, 1927, 2971, 8024, 4495, 3833,  679, 1927, 2415,  511,  782, 4495,\n",
      "         3766, 3300, 2506, 2961, 8024, 3680,  671, 1921, 6963,  679,  833, 7028,\n",
      "         3341,  511, 3140,  754, 2904, 2773, 6847, 1862, 4638,  782, 8024, 4495,\n",
      "         1462, 1728, 3634, 5743, 1896,  511,  862, 3198,  862, 1765, 8024,  872,\n",
      "         6963, 6206, 3209, 4635, 8024,  872, 3221, 3833, 5314, 5632, 2346, 4692,\n",
      "         4638, 8024, 1166, 2828, 1166,  782, 4638, 6397,  817, 4692, 2533, 1922,\n",
      "         7028, 8024, 1127,  752, 1372, 6206,  754, 2552, 3187, 2700, 8024, 2218,\n",
      "          679, 2553, 6369, 6772, 1922, 1914,  511, 2694, 5442, 1372, 3300, 1898,\n",
      "         1898, 1500, 1386, 8024, 3255, 5442, 1316, 3300, 1283, 6662,  674, 6662,\n",
      "          511, 2750, 2680, 3221, 5164,  817, 3353, 7770, 4638, 1951,  890, 1501,\n",
      "         8024,  671, 3190, 1168, 3309, 3926,  802, 8024, 2553, 2137,  985, 6820,\n",
      "          679, 6629,  511,  679, 3221, 3766,  749,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])} tensor([2])\n",
      "c3fb4257-64ed-4c43-bc13-344d890cf229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/7534 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情感解读\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval() # 设置模型为评估/测试模式\n",
    "valid_dataset = MyDataset(processed_expose_train_labeled_valid_path, 'valid')\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(valid_loader):\n",
    "        print(data, label)\n",
    "        print(data['id'][0])\n",
    "        output = model(data)\n",
    "#         print(output)\n",
    "#         print(torch.sigmoid(output))\n",
    "#         sigmoid = nn.Sigmoid()\n",
    "#         print(sigmoid(output))\n",
    "        predict_list = F.softmax(output, dim=1).tolist()[0]\n",
    "        predict_index = np.argmax(predict_list)\n",
    "        predict_label = doctype_list[predict_index]\n",
    "        print(predict_label)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2331it [00:00, 83173.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"ee137ac3-c2a2-4aba-a517-36840ffd2f1a\", \"category\": 11, \"doctype\": \"人物专栏\", \"paragraphs_num\": 1, \"pic_num\": 0, \"source\": \"\", \"words_len\": 2142, \"text\": \"经过22年的时间漂流，《尘埃落定》是如何留下来的？阿来在杭州这么说。乡愁与告别：从《尘埃落定》到《机村史诗》李敬泽称此为乡愁：“当我们热爱田园时，我们对自然、大地、村庄的那个热爱，实际上不仅是因为很多东西正在逝去，也是因为那些过去里包含着的，我们认为我们生命当中非常珍贵的、必须珍视的价值在逝去。虽然它写了那一段特定历史中的故事，但其实你打开看到的是，人处在那样一个史诗般的天真年代，人与自然、与社会、与自己的关系。《尘埃落定》写了20世纪前50年的故乡，后来他又有了《机村史诗》，写后50年的故乡。20年前后：文学经典对读者的意义《尘埃落定》1994年完稿，1998年首次出版，2000年获第五届茅盾文学奖。对谈中，阿来也说：“写完《尘埃落定》这本书，我离开它就更有勇气了，至少我们俩和解了，或者说两清了，那些纠缠都放下了。这也意味着这部书经过22年的时间漂流，已经正在成为现代的一个节点，它会继续被一代代的人阅读。他说，我们中国人习惯于读大历史，对于鸦片战争、辛亥革命非常熟，“但我们不知道这个地方的历史，不知道我的那个县、我的那个村、我的那个乡，它的历史。”书中故事所代表的久远的乡愁，对故乡与山川大地匹配的英雄浪漫的气质，后来阿来觉得把这些找回来了。”李敬泽说，“一开始，我们会看书中那段历史，看土司制度的命运，看整个藏区经受的巨大历史变革等。\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# with open(\"data/processed_train_expose_labeled.json\", \"r\", encoding='utf-8') as input_file:\n",
    "#     for line in tqdm(input_file):\n",
    "#         json_data = json.loads(line)\n",
    "#         if json_data['id'] == 'ee137ac3-c2a2-4aba-a517-36840ffd2f1a':\n",
    "#             print(line)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, ?it/s]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01922558806836605, 0.12085458636283875, 0.011475713923573494, 0.11851993203163147, 0.2810468077659607, 0.18804335594177246, 0.008331549353897572, 0.18468160927295685, 0.017482150346040726, 0.050338730216026306]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model.eval() # 设置模型为评估/测试模式\n",
    "# valid_dataset = MyDataset(\"data/test.txt\", 'test')\n",
    "# valid_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "# with torch.no_grad():\n",
    "#     for data, label in tqdm(valid_loader):\n",
    "# #         print(data, label)\n",
    "#         output = model(data)\n",
    "# #         print(output)\n",
    "# #         print(torch.sigmoid(output))\n",
    "#         print(F.softmax(output, dim=1).tolist()[0])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45285it [00:00, 90916.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 45285/45285 [28:01<00:00, 26.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_doctype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001a2f1-714e-4eca-8d26-d0b173d8d327</th>\n",
       "      <td>情感解读</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00028139-6f2c-4321-b3e0-5ddf7c9af4eb</th>\n",
       "      <td>作品分析</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00063b7f-03db-430b-857c-b127a970422c</th>\n",
       "      <td>行业解读</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0006fe16-ae5d-432b-8fbd-f0653200069c</th>\n",
       "      <td>深度事件</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008fcff-3bd0-4a61-acb3-d995c8871768</th>\n",
       "      <td>深度事件</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     predict_doctype\n",
       "0001a2f1-714e-4eca-8d26-d0b173d8d327            情感解读\n",
       "00028139-6f2c-4321-b3e0-5ddf7c9af4eb            作品分析\n",
       "00063b7f-03db-430b-857c-b127a970422c            行业解读\n",
       "0006fe16-ae5d-432b-8fbd-f0653200069c            深度事件\n",
       "0008fcff-3bd0-4a61-acb3-d995c8871768            深度事件"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # 设置模型为评估/测试模式\n",
    "test_dataset = MyDataset(processed_expose_test_path, 'test')\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "predict_doctype = {}\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(test_loader):\n",
    "#         print(data, label)\n",
    "        id = data['id'][0]\n",
    "        output = model(data)\n",
    "#         print(output)\n",
    "#         print(torch.sigmoid(output))\n",
    "#         sigmoid = nn.Sigmoid()\n",
    "#         print(sigmoid(output))\n",
    "        predict_list = F.softmax(output, dim=1).tolist()[0]\n",
    "        predict_index = np.argmax(predict_list)\n",
    "        predict_label = doctype_list[predict_index]\n",
    "#         print(predict_label)\n",
    "        predict_doctype[id] = predict_label\n",
    "predict_data = {'predict_doctype' : predict_doctype}\n",
    "df = pd.DataFrame(predict_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_doctype</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001a2f1-714e-4eca-8d26-d0b173d8d327</th>\n",
       "      <td>情感解读</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00028139-6f2c-4321-b3e0-5ddf7c9af4eb</th>\n",
       "      <td>作品分析</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00063b7f-03db-430b-857c-b127a970422c</th>\n",
       "      <td>行业解读</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0006fe16-ae5d-432b-8fbd-f0653200069c</th>\n",
       "      <td>深度事件</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0008fcff-3bd0-4a61-acb3-d995c8871768</th>\n",
       "      <td>深度事件</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     predict_doctype\n",
       "id                                                  \n",
       "0001a2f1-714e-4eca-8d26-d0b173d8d327            情感解读\n",
       "00028139-6f2c-4321-b3e0-5ddf7c9af4eb            作品分析\n",
       "00063b7f-03db-430b-857c-b127a970422c            行业解读\n",
       "0006fe16-ae5d-432b-8fbd-f0653200069c            深度事件\n",
       "0008fcff-3bd0-4a61-acb3-d995c8871768            深度事件"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.name = 'id'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission_train_first_predict.csv\")\n",
    "# 0.398140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████▋                            | 297712/500000 [3:00:28<2:02:23, 27.55it/s]"
     ]
    }
   ],
   "source": [
    "model.eval() # 设置模型为评估/测试模式\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_path)\n",
    "# count = 0\n",
    "with torch.no_grad():\n",
    "    with open(processed_expose_train_unlabel_path, \"r\", encoding='utf-8') as unlabel_file, \\\n",
    "         open(processed_expose_train_unlabel_predict_list_path, \"w\", encoding='utf-8') as unlabel_predict_list_file:\n",
    "        for line in tqdm(unlabel_file, total=500000):\n",
    "            json_data = json.loads(line)\n",
    "            data, label = MyDataset.get_data_label(tokenizer, copy.deepcopy(json_data), \"test\")\n",
    "            data['category'] = torch.tensor(data['category']).reshape(1, -1)\n",
    "            data['paragraphs_num'] = torch.tensor(data['paragraphs_num']).reshape(1, -1)\n",
    "            data['source'] = torch.tensor(data['source']).reshape(1, -1)\n",
    "            data['words_len'] = torch.tensor(data['words_len']).reshape(1, -1)\n",
    "            data['input_ids'] = data['input_ids'].reshape(1, -1)\n",
    "            data['token_type_ids'] = data['token_type_ids'].reshape(1, -1)\n",
    "            data['attention_mask'] = data['attention_mask'].reshape(1, -1)\n",
    "#             print(data)\n",
    "#             print(label)\n",
    "            output = model(data)\n",
    "            predict_list = F.softmax(output, dim=1).tolist()[0]\n",
    "#             print(predict_list)\n",
    "#             count += 1\n",
    "#             if count>=2:\n",
    "#                 break\n",
    "#             print(json_data)\n",
    "#             break\n",
    "            json_data['predict_list'] = predict_list\n",
    "            unlabel_predict_list_file.write(f\"{json.dumps(json_data, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500000it [00:02, 200751.81it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(processed_expose_train_unlabel_predict_list_path, \"r\", encoding='utf-8') as unlabel_predict_list_file:\n",
    "    for line in tqdm(unlabel_predict_list_file):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2021_huawei_ai_match.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
